{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfTmQLpU0SZF",
        "outputId": "74acf33c-7cb6-4005-8573-3e379136554c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All imports successful!\n",
            "Available datasets:\n",
            "1. plants\n",
            "2. aid\n",
            "3. lc25000\n",
            "Select dataset (enter number or name): 3\n",
            "Selected dataset: lc25000\n",
            "Are you running this code in Google Colab or locally? (colab/local): colab\n",
            "âœ… Configuration set for LC25000 dataset!\n",
            "Downloading lc25000 dataset: https://www.kaggle.com/datasets/javaidahmadwani/lc25000\n",
            "Dataset URL: https://www.kaggle.com/datasets/javaidahmadwani/lc25000\n",
            "Downloading lc25000.zip to ./lc25000-datasets/lc25000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.76G/1.76G [00:51<00:00, 36.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… LC25000 dataset downloaded successfully!\n",
            "âœ… Data loading functions defined!\n",
            "âœ… Feature extraction utilities defined!\n",
            "âœ… SSN implementation complete!\n",
            "âœ… GWO-ANN-SSN Feature Selector class defined!\n",
            "ðŸš€ Starting Unified GWO-ANN-SSN Pipeline\n",
            "============================================================\n",
            "ðŸš€ Starting feature extraction pipeline...\n",
            "ðŸ“‚ Loading LC25000 dataset...\n",
            "âœ… Loaded 2499 images from ./lc25000-datasets/lc25000/lung_colon_image_set/Test Set\n",
            "âœ… Loaded 12500 images from ./lc25000-datasets/lc25000/lung_colon_image_set/Train and Validation Set\n",
            "ðŸŽ¯ Classes found: 5\n",
            "ðŸ“Š Dataset sizes - Train: 10000, Val: 2500, Test: 2499\n",
            "ðŸ”§ Extracting features for train...\n",
            "    Processed 10/79 batches\n",
            "    Processed 20/79 batches\n",
            "    Processed 30/79 batches\n",
            "    Processed 40/79 batches\n",
            "    Processed 50/79 batches\n",
            "    Processed 60/79 batches\n",
            "    Processed 70/79 batches\n",
            "    Processed 79/79 batches\n",
            "ðŸ’¾ Saved features for train: (10000, 512)\n",
            "ðŸ”§ Extracting features for val...\n",
            "    Processed 10/20 batches\n",
            "    Processed 20/20 batches\n",
            "ðŸ’¾ Saved features for val: (2500, 512)\n",
            "ðŸ”§ Extracting features for test...\n",
            "    Processed 10/20 batches\n",
            "    Processed 20/20 batches\n",
            "ðŸ’¾ Saved features for test: (2499, 512)\n",
            "âœ… Feature extraction completed!\n",
            "ðŸ”§ Standardizing features...\n",
            "ðŸ“Š Applying PCA...\n",
            "âœ… PCA completed: 325 features (from 512)\n",
            "ðŸ’¾ Saved PCA and Scaler\n",
            "âœ… Data preprocessing completed!\n",
            "ðŸš€ Starting EFFICIENT GWO-ANN-SSN Feature Selection...\n",
            "============================================================\n",
            "âš¡ RUNNING IN EFFICIENT MODE âš¡\n",
            " Â  â€¢ Population: 8\n",
            " Â  â€¢ Generations: 10\n",
            " Â  â€¢ Evaluation Epochs: 5\n",
            "------------------------------------------------------------\n",
            "ðŸš€ Starting GWO-ANN-SSN Feature Selection\n",
            "============================================================\n",
            "\n",
            "ðŸ”„ Generation 1/10\n",
            " Â  âœ… New alpha! Fitness: 0.1759, Accuracy: 0.9528, Features: 156\n",
            " Â  ðŸ“Š Stats - Best: 0.1759, Avg: 0.1923, Diversity: 0.4019\n",
            "\n",
            "ðŸ”„ Generation 2/10\n",
            " Â  âœ… New alpha! Fitness: 0.1700, Accuracy: 0.9692, Features: 166\n",
            " Â  ðŸ“Š Stats - Best: 0.1700, Avg: 0.1807, Diversity: 0.3523\n",
            "\n",
            "ðŸ”„ Generation 3/10\n",
            " Â  âœ… New alpha! Fitness: 0.1635, Accuracy: 0.9700, Features: 152\n",
            " Â  ðŸ“Š Stats - Best: 0.1635, Avg: 0.1699, Diversity: 0.2978\n",
            "\n",
            "ðŸ”„ Generation 4/10\n",
            " Â  âœ… New alpha! Fitness: 0.1563, Accuracy: 0.9600, Features: 139\n",
            " Â  ðŸ“Š Stats - Best: 0.1563, Avg: 0.1671, Diversity: 0.2606\n",
            "\n",
            "ðŸ”„ Generation 5/10\n",
            " Â  âœ… New alpha! Fitness: 0.1449, Accuracy: 0.9668, Features: 133\n",
            " Â  ðŸ“Š Stats - Best: 0.1449, Avg: 0.1572, Diversity: 0.2211\n",
            "\n",
            "ðŸ”„ Generation 6/10\n",
            " Â  âœ… New alpha! Fitness: 0.1414, Accuracy: 0.9672, Features: 128\n",
            " Â  ðŸ“Š Stats - Best: 0.1414, Avg: 0.1532, Diversity: 0.1641\n",
            "\n",
            "ðŸ”„ Generation 7/10\n",
            " Â  âœ… New alpha! Fitness: 0.1379, Accuracy: 0.9644, Features: 123\n",
            " Â  ðŸ“Š Stats - Best: 0.1379, Avg: 0.1460, Diversity: 0.1292\n",
            "\n",
            "ðŸ”„ Generation 8/10\n",
            " Â  âœ… New alpha! Fitness: 0.1363, Accuracy: 0.9676, Features: 124\n",
            " Â  ðŸ“Š Stats - Best: 0.1363, Avg: 0.1414, Diversity: 0.1002\n",
            "\n",
            "ðŸ”„ Generation 9/10\n",
            " Â  ðŸ“Š Stats - Best: 0.1383, Avg: 0.1396, Diversity: 0.0842\n",
            "\n",
            "ðŸ”„ Generation 10/10\n",
            " Â  ðŸ“Š Stats - Best: 0.1378, Avg: 0.1402, Diversity: 0.0571\n",
            "\n",
            "============================================================\n",
            "ðŸŽ¯ GWO-ANN-SSN COMPLETED\n",
            "============================================================\n",
            "Best Fitness: 0.1363\n",
            "Best Accuracy: 0.9676\n",
            "Selected Features: 124/325\n",
            "Final Generation: 10\n",
            "âœ… GWO-ANN-SSN feature selection completed!\n",
            "ðŸŽ¯ Final selection: 124 features out of 325\n",
            "\n",
            "â„¹ï¸ Restored EVAL_EPOCHS to original value (10)\n",
            "\n",
            "ðŸŽ¯ Training Final Model with Selected Features...\n",
            "============================================================\n",
            "ðŸ“Š Final training set: (12500, 124)\n",
            "ðŸ“Š Test set: (2499, 124)\n",
            "ðŸ”§ Selected features: 124\n",
            "ðŸ”§ Training final model on combined train+val data for 100 epochs...\n",
            "Epoch 1/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.7993 - loss: 1.5477\n",
            "Epoch 2/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9324 - loss: 1.0598\n",
            "Epoch 3/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9522 - loss: 0.9390\n",
            "Epoch 4/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9591 - loss: 0.8295\n",
            "Epoch 5/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9650 - loss: 0.7268\n",
            "Epoch 6/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9676 - loss: 0.6438\n",
            "Epoch 7/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9709 - loss: 0.5604\n",
            "Epoch 8/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9703 - loss: 0.4973\n",
            "Epoch 9/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9737 - loss: 0.4438\n",
            "Epoch 10/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9719 - loss: 0.4076\n",
            "Epoch 11/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9735 - loss: 0.3678\n",
            "Epoch 12/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9719 - loss: 0.3633\n",
            "Epoch 13/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9681 - loss: 0.3754\n",
            "Epoch 14/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9707 - loss: 0.3662\n",
            "Epoch 15/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9724 - loss: 0.3519\n",
            "Epoch 16/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9722 - loss: 0.3519\n",
            "Epoch 17/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9767 - loss: 0.3250\n",
            "Epoch 18/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9762 - loss: 0.3173\n",
            "Epoch 19/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9743 - loss: 0.3236\n",
            "Epoch 20/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9756 - loss: 0.3107\n",
            "Epoch 21/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9782 - loss: 0.2988\n",
            "Epoch 22/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9760 - loss: 0.3116\n",
            "Epoch 23/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9776 - loss: 0.3051\n",
            "Epoch 24/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9750 - loss: 0.3245\n",
            "Epoch 25/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9792 - loss: 0.3040\n",
            "Epoch 26/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9771 - loss: 0.2996\n",
            "Epoch 27/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9750 - loss: 0.3157\n",
            "Epoch 28/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9786 - loss: 0.2963\n",
            "Epoch 29/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9807 - loss: 0.2897\n",
            "Epoch 30/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9779 - loss: 0.3011\n",
            "Epoch 31/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9823 - loss: 0.2756\n",
            "Epoch 32/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9837 - loss: 0.2696\n",
            "Epoch 33/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9806 - loss: 0.2800\n",
            "Epoch 34/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9809 - loss: 0.2754\n",
            "Epoch 35/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9779 - loss: 0.2743\n",
            "Epoch 36/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9816 - loss: 0.2865\n",
            "Epoch 37/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9814 - loss: 0.2950\n",
            "Epoch 38/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9820 - loss: 0.2888\n",
            "Epoch 39/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9806 - loss: 0.3010\n",
            "Epoch 40/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9805 - loss: 0.2855\n",
            "Epoch 41/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9816 - loss: 0.2782\n",
            "Epoch 42/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9807 - loss: 0.2837\n",
            "Epoch 43/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9807 - loss: 0.2815\n",
            "Epoch 44/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9861 - loss: 0.2708\n",
            "Epoch 45/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9861 - loss: 0.2529\n",
            "Epoch 46/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9854 - loss: 0.2534\n",
            "Epoch 47/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9841 - loss: 0.2662\n",
            "Epoch 48/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9831 - loss: 0.2660\n",
            "Epoch 49/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9825 - loss: 0.2725\n",
            "Epoch 50/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9827 - loss: 0.2757\n",
            "Epoch 51/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9850 - loss: 0.2622\n",
            "Epoch 52/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9854 - loss: 0.2766\n",
            "Epoch 53/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9849 - loss: 0.2682\n",
            "Epoch 54/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9869 - loss: 0.2594\n",
            "Epoch 55/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9893 - loss: 0.2368\n",
            "Epoch 56/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9850 - loss: 0.2532\n",
            "Epoch 57/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9839 - loss: 0.2643\n",
            "Epoch 58/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9838 - loss: 0.2607\n",
            "Epoch 59/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9802 - loss: 0.2721\n",
            "Epoch 60/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9808 - loss: 0.2668\n",
            "Epoch 61/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9851 - loss: 0.2574\n",
            "Epoch 62/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9870 - loss: 0.2450\n",
            "Epoch 63/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9873 - loss: 0.2428\n",
            "Epoch 64/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9840 - loss: 0.2463\n",
            "Epoch 65/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9845 - loss: 0.2468\n",
            "Epoch 66/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9865 - loss: 0.2423\n",
            "Epoch 67/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9841 - loss: 0.2550\n",
            "Epoch 68/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9880 - loss: 0.2411\n",
            "Epoch 69/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9849 - loss: 0.2475\n",
            "Epoch 70/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9847 - loss: 0.2500\n",
            "Epoch 71/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9864 - loss: 0.2436\n",
            "Epoch 72/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9843 - loss: 0.2535\n",
            "Epoch 73/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9857 - loss: 0.2453\n",
            "Epoch 74/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9864 - loss: 0.2426\n",
            "Epoch 75/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9859 - loss: 0.2331\n",
            "Epoch 76/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9869 - loss: 0.2241\n",
            "Epoch 77/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9872 - loss: 0.2236\n",
            "Epoch 78/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9871 - loss: 0.2221\n",
            "Epoch 79/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9871 - loss: 0.2242\n",
            "Epoch 80/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9856 - loss: 0.2308\n",
            "Epoch 81/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9846 - loss: 0.2456\n",
            "Epoch 82/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9853 - loss: 0.2411\n",
            "Epoch 83/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9851 - loss: 0.2466\n",
            "Epoch 84/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9864 - loss: 0.2392\n",
            "Epoch 85/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9854 - loss: 0.2332\n",
            "Epoch 86/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9861 - loss: 0.2306\n",
            "Epoch 87/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9900 - loss: 0.2211\n",
            "Epoch 88/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9884 - loss: 0.2215\n",
            "Epoch 89/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9867 - loss: 0.2198\n",
            "Epoch 90/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9875 - loss: 0.2252\n",
            "Epoch 91/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9861 - loss: 0.2261\n",
            "Epoch 92/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9869 - loss: 0.2236\n",
            "Epoch 93/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9885 - loss: 0.2149\n",
            "Epoch 94/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9852 - loss: 0.2333\n",
            "Epoch 95/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9861 - loss: 0.2303\n",
            "Epoch 96/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9842 - loss: 0.2304\n",
            "Epoch 97/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9866 - loss: 0.2289\n",
            "Epoch 98/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9850 - loss: 0.2299\n",
            "Epoch 99/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9862 - loss: 0.2276\n",
            "Epoch 100/100\n",
            "\u001b[1m196/196\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9874 - loss: 0.2263\n",
            "âœ… Final model training completed!\n",
            "\n",
            "ðŸ“ˆ Evaluating on completely held-out TEST set...\n",
            "============================================================\n",
            "ðŸ† VALIDATION Accuracy (for reference): 0.9968\n",
            "ðŸŽ¯ TEST Accuracy (true performance): 0.9768\n",
            "\n",
            "============================================================\n",
            "ðŸŽ¯ FINAL GWO-ANN-SSN RESULTS - LC25000\n",
            "============================================================\n",
            "ðŸ“Š Dataset Usage:\n",
            " Â  â€¢ Train: 10000 samples (feature selection)\n",
            " Â  â€¢ Validation: 2500 samples (fitness evaluation)\n",
            " Â  â€¢ Test: 2499 samples (FINAL evaluation)\n",
            " Â  â€¢ Final Training: 12500 samples (train+val)\n",
            "\n",
            "ðŸŽ¯ Performance Metrics:\n",
            " Â  â€¢ Best Fitness: 0.1363\n",
            " Â  â€¢ Validation Accuracy: 0.9968\n",
            " Â  â€¢ ðŸ† TEST Accuracy: 0.9768 â† TRUE PERFORMANCE\n",
            "\n",
            "ðŸ”§ Feature Selection:\n",
            " Â  â€¢ Selected Features: 124/325\n",
            " Â  â€¢ Feature Reduction: 61.8%\n",
            "\n",
            "ðŸ’¾ Results saved to: ./saved_features_lc25000/gwo_ann_ssn_results.pkl\n",
            "âœ… Unified pipeline completed successfully!\n",
            "\n",
            "============================================================\n",
            "ðŸ“Š RESULTS CONSOLIDATION\n",
            "============================================================\n",
            "ðŸ’¾ Results appended to: gwo_ann_ssn_datasets_results.json\n",
            "ðŸ“‹ Summary updated in: gwo_ann_ssn_datasets_summary.txt\n",
            "\n",
            "ðŸ“Š LC25000 RESULTS:\n",
            "--------------------------------------------------\n",
            "Test Accuracy:       0.9768\n",
            "Validation Accuracy: 0.9968\n",
            "Selected Features:   124/325\n",
            "Feature Reduction:   61.8%\n",
            "Best Fitness:        0.1363\n",
            "\n",
            "âœ… All results consolidated successfully!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Installations and Imports\n",
        "# =================================\n",
        "!pip install opendatasets kaggle pandas tensorflow scikit-learn opencv-python -q\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import gc\n",
        "import pickle\n",
        "import json\n",
        "import opendatasets as od\n",
        "import pandas as pd # Added pandas for timestamp in consolidation\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='keras.*')\n",
        "\n",
        "print(\"âœ… All imports successful!\")\n",
        "\n",
        "# Cell 2: Configuration and Dataset Selection\n",
        "# ===========================================\n",
        "def get_dataset_choice():\n",
        "    print(\"Available datasets:\")\n",
        "    print(\"1. plants\")\n",
        "    print(\"2. aid\")\n",
        "    print(\"3. lc25000\")\n",
        "\n",
        "    while True:\n",
        "        choice = input(\"Select dataset (enter number or name): \").strip().lower()\n",
        "\n",
        "        # Handle numeric input\n",
        "        if choice == \"1\":\n",
        "            return \"plants\"\n",
        "        elif choice == \"2\":\n",
        "            return \"aid\"\n",
        "        elif choice == \"3\":\n",
        "            return \"lc25000\"\n",
        "        elif choice in [\"plants\", \"aid\", \"lc25000\"]:\n",
        "            return choice\n",
        "        else:\n",
        "            print(\"Invalid choice. Please enter 1, 2, 3, or the dataset name.\")\n",
        "\n",
        "DATASET_CHOICE = get_dataset_choice()\n",
        "print(f\"Selected dataset: {DATASET_CHOICE}\")\n",
        "\n",
        "# Ask user about their environment\n",
        "def get_environment():\n",
        "    while True:\n",
        "        env = input(\"Are you running this code in Google Colab or locally? (colab/local): \").lower().strip()\n",
        "        if env in ['colab', 'local']:\n",
        "            return env\n",
        "        else:\n",
        "            print(\"Please enter either 'colab' or 'local'\")\n",
        "\n",
        "environment = get_environment()\n",
        "\n",
        "# Dataset-specific configurations\n",
        "DATASET_CONFIGS = {\n",
        "    \"plants\": {\n",
        "        \"dataset_url\": \"https://www.kaggle.com/datasets/yudhaislamisulistya/plants-type-datasets\",\n",
        "        \"download_folder\": \"./plant-datasets\",\n",
        "        \"train_path\": \"\",\n",
        "        \"val_path\": \"\",\n",
        "        \"test_path\": \"\",\n",
        "        \"features_dir\": \"./saved_features_plants\",\n",
        "        \"split_type\": \"pre_split\"\n",
        "    },\n",
        "    \"aid\": {\n",
        "        \"dataset_url\": \"https://www.kaggle.com/datasets/jiayuanchengala/aid-scene-classification-datasets\",\n",
        "        \"download_folder\": \"./aid-datasets\",\n",
        "        \"data_root\": \"\",\n",
        "        \"features_dir\": \"./saved_features_aid\",\n",
        "        \"split_type\": \"70_10_20\"\n",
        "    },\n",
        "    \"lc25000\": {\n",
        "        \"dataset_url\": \"https://www.kaggle.com/datasets/javaidahmadwani/lc25000\",\n",
        "        \"download_folder\": \"./lc25000-datasets\",\n",
        "        \"data_root\": \"\",\n",
        "        \"features_dir\": \"./saved_features_lc25000\",\n",
        "        \"split_type\": \"lc25000_split\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Set paths based on environment\n",
        "if environment == \"colab\":\n",
        "    # Colab paths\n",
        "    DATASET_CONFIGS[\"plants\"][\"train_path\"] = \"/content/plant-datasets/plants-type-datasets/split_ttv_dataset_type_of_plants/Train_Set_Folder\"\n",
        "    DATASET_CONFIGS[\"plants\"][\"val_path\"] = \"/content/plant-datasets/plants-type-datasets/split_ttv_dataset_type_of_plants/Validation_Set_Folder\"\n",
        "    DATASET_CONFIGS[\"plants\"][\"test_path\"] = \"/content/plant-datasets/plants-type-datasets/split_ttv_dataset_type_of_plants/Test_Set_Folder\"\n",
        "    DATASET_CONFIGS[\"aid\"][\"data_root\"] = \"./aid-datasets/aid-scene-classification-datasets/AID\"\n",
        "    DATASET_CONFIGS[\"lc25000\"][\"data_root\"] = \"./lc25000-datasets/lc25000/lung_colon_image_set\"\n",
        "\n",
        "else:  # local environment\n",
        "    print(\"\\nFor local execution, please ensure you have the following folders in your current directory:\")\n",
        "    print(\"1. split_ttv_dataset_type_of_plants\")\n",
        "    print(\"2. AID\")\n",
        "    print(\"3. lung_colon_image_set\")\n",
        "\n",
        "    # Local paths\n",
        "    DATASET_CONFIGS[\"plants\"][\"train_path\"] = \"./split_ttv_dataset_type_of_plants/Train_Set_Folder\"\n",
        "    DATASET_CONFIGS[\"plants\"][\"val_path\"] = \"./split_ttv_dataset_type_of_plants/Validation_Set_Folder\"\n",
        "    DATASET_CONFIGS[\"plants\"][\"test_path\"] = \"./split_ttv_dataset_type_of_plants/Test_Set_Folder\"\n",
        "    DATASET_CONFIGS[\"aid\"][\"data_root\"] = \"./AID\"\n",
        "    DATASET_CONFIGS[\"lc25000\"][\"data_root\"] = \"./lung_colon_image_set\"\n",
        "\n",
        "\n",
        "# Get current config\n",
        "CONFIG = DATASET_CONFIGS[DATASET_CHOICE]\n",
        "\n",
        "# Common parameters\n",
        "IMG_SIZE = (128, 128)\n",
        "VGG_TARGET_SIZE = (224, 224)\n",
        "PCA_VARIANCE = 0.95\n",
        "\n",
        "# GA Parameters (kept for reference, but GWO will use its own)\n",
        "GA_POPULATION_SIZE = 10\n",
        "GA_MAX_GENERATION = 40\n",
        "GA_CROSSOVER_RATE = 0.8\n",
        "GA_MUTATION_RATE = 0.2\n",
        "\n",
        "# Training Parameters\n",
        "FEATURE_EXTRACTION_BATCH_SIZE = 128\n",
        "BATCH_SIZE = 64\n",
        "EVAL_EPOCHS = 10\n",
        "FINAL_EPOCHS = 100 # Increased for better convergence\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Set seeds\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "# Create features directory\n",
        "os.makedirs(CONFIG['features_dir'], exist_ok=True)\n",
        "\n",
        "print(f\"âœ… Configuration set for {DATASET_CHOICE.upper()} dataset!\")\n",
        "\n",
        "# Cell 3: Dataset Download\n",
        "# ========================\n",
        "def download_dataset():\n",
        "    \"\"\"Download dataset based on configuration\"\"\"\n",
        "    # âš ï¸ REPLACE WITH YOUR ACTUAL KAGGLE CREDENTIALS âš ï¸\n",
        "    kaggle_credentials = {\"username\": 'yuvanrajvengaladas', \"key\": '6fd3ece8111002ccca9494a6d7e6212e'}\n",
        "\n",
        "    with open(\"kaggle.json\", \"w\") as f:\n",
        "        json.dump(kaggle_credentials, f)\n",
        "\n",
        "    try:\n",
        "        with open(\"kaggle.json\", 'r') as f:\n",
        "            credentials = json.load(f)\n",
        "        os.environ['KAGGLE_USERNAME'] = credentials['username']\n",
        "        os.environ['KAGGLE_KEY'] = credentials['key']\n",
        "\n",
        "        print(f\"Downloading {DATASET_CHOICE} dataset: {CONFIG['dataset_url']}\")\n",
        "        od.download(CONFIG['dataset_url'], data_dir=CONFIG['download_folder'])\n",
        "        print(f\"âœ… {DATASET_CHOICE.upper()} dataset downloaded successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error downloading dataset: {e}\")\n",
        "        print(\"ðŸ‘‰ Please ensure your kaggle.json file has correct credentials\")\n",
        "\n",
        "\n",
        "# Download the dataset\n",
        "if environment == \"colab\":\n",
        "    download_dataset()\n",
        "\n",
        "# Cell 4: Data Loading and Splitting Functions\n",
        "# ============================================\n",
        "def load_images_from_folder(folder, img_size=IMG_SIZE, max_samples=None):\n",
        "    \"\"\"Load images from folder structure (for pre-split datasets)\"\"\"\n",
        "    X, y, mapping = [], [], {}\n",
        "    class_folders = sorted([f for f in os.listdir(folder) if os.path.isdir(os.path.join(folder, f))])\n",
        "\n",
        "    for idx, cname in enumerate(class_folders):\n",
        "        mapping[idx] = cname\n",
        "        files = glob(os.path.join(folder, cname, \"*.jpg\")) + glob(os.path.join(folder, cname, \"*.png\")) + glob(os.path.join(folder, cname, \"*.jpeg\"))\n",
        "\n",
        "        if max_samples:\n",
        "            files = files[:max_samples]\n",
        "\n",
        "        for f in files:\n",
        "            img = cv2.imread(f)\n",
        "            if img is None:\n",
        "                continue\n",
        "            img = cv2.resize(img, img_size)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            X.append(img)\n",
        "            y.append(idx)\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y).reshape(-1, 1)\n",
        "    print(f\"âœ… Loaded {len(X)} images from {folder}\")\n",
        "    return X, y, mapping\n",
        "\n",
        "def get_all_filepaths_and_labels(data_root):\n",
        "    \"\"\"Gather all image file paths and labels (for non-split datasets)\"\"\"\n",
        "    all_files, all_labels, mapping = [], [], {}\n",
        "    class_folders = sorted([f for f in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, f))])\n",
        "\n",
        "    for idx, cname in enumerate(class_folders):\n",
        "        mapping[idx] = cname\n",
        "        class_dir = os.path.join(data_root, cname)\n",
        "        files = glob(os.path.join(class_dir, \"*.jpg\")) + glob(os.path.join(class_dir, \"*.png\"))\n",
        "        all_files.extend(files)\n",
        "        all_labels.extend([idx] * len(files))\n",
        "\n",
        "    print(f\"ðŸ“ Found {len(all_files)} images across {len(mapping)} classes.\")\n",
        "    return np.array(all_files), np.array(all_labels), mapping\n",
        "\n",
        "def load_images_from_paths(paths, labels, img_size=IMG_SIZE, max_samples=None):\n",
        "    \"\"\"Load images from file paths\"\"\"\n",
        "    X, valid_labels = [], []\n",
        "    if max_samples:\n",
        "        indices = np.random.choice(len(paths), min(max_samples, len(paths)), replace=False)\n",
        "        paths = paths[indices]\n",
        "        labels = labels[indices]\n",
        "\n",
        "    for i, f in enumerate(paths):\n",
        "        img = cv2.imread(f)\n",
        "        if img is None:\n",
        "            continue\n",
        "        img = cv2.resize(img, img_size)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        X.append(img)\n",
        "        valid_labels.append(labels[i])\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(valid_labels).reshape(-1, 1)\n",
        "    print(f\"âœ… Loaded {len(X)} images from paths.\")\n",
        "    return X, y\n",
        "\n",
        "def load_dataset_plants():\n",
        "    \"\"\"Load Plants dataset (pre-split)\"\"\"\n",
        "    # Reduced sample counts for efficiency/testing purposes\n",
        "    X_train, y_train, mapping = load_images_from_folder(CONFIG['train_path'], max_samples=1000)\n",
        "    X_val, y_val, _ = load_images_from_folder(CONFIG['val_path'], max_samples=300)\n",
        "    X_test, y_test, _ = load_images_from_folder(CONFIG['test_path'], max_samples=300)\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, mapping\n",
        "\n",
        "def load_dataset_aid():\n",
        "    \"\"\"Load AID dataset and split 70-10-20\"\"\"\n",
        "    X_paths, y_labels, mapping = get_all_filepaths_and_labels(CONFIG['data_root'])\n",
        "\n",
        "    # Split: 70% Train, 15% Validation, 15% Test\n",
        "    X_train_paths, X_temp_paths, y_train_labels, y_temp_labels = train_test_split(\n",
        "        X_paths, y_labels, test_size=0.3, random_state=RANDOM_SEED, stratify=y_labels\n",
        "    )\n",
        "    X_val_paths, X_test_paths, y_val_labels, y_test_labels = train_test_split(\n",
        "        X_temp_paths, y_temp_labels, test_size=0.5, random_state=RANDOM_SEED, stratify=y_temp_labels\n",
        "    )\n",
        "\n",
        "    # Load images (with reduced max_samples for efficiency)\n",
        "    X_train, y_train = load_images_from_paths(X_train_paths, y_train_labels, max_samples=2000)\n",
        "    X_val, y_val = load_images_from_paths(X_val_paths, y_val_labels, max_samples=500)\n",
        "    X_test, y_test = load_images_from_paths(X_test_paths, y_test_labels, max_samples=500)\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, mapping\n",
        "\n",
        "def load_dataset_lc25000():\n",
        "    \"\"\"Load LC25000 dataset with custom split\"\"\"\n",
        "    test_folder = os.path.join(CONFIG['data_root'], \"Test Set\")\n",
        "    train_val_folder = os.path.join(CONFIG['data_root'], \"Train and Validation Set\")\n",
        "\n",
        "    # Load test set (reduced max_samples for efficiency)\n",
        "    X_test, y_test, mapping = load_images_from_folder(test_folder, max_samples=500)\n",
        "\n",
        "    # Load train-val set and split 80-20 (reduced max_samples for efficiency)\n",
        "    X_train_val, y_train_val, mapping = load_images_from_folder(train_val_folder, max_samples=2500)\n",
        "\n",
        "    # Split train-val into 80% train, 20% validation\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_val, y_train_val, test_size=0.2, random_state=RANDOM_SEED, stratify=y_train_val\n",
        "    )\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, mapping\n",
        "\n",
        "# Main data loading function\n",
        "def load_datasets():\n",
        "    \"\"\"Main function to load datasets based on choice\"\"\"\n",
        "    print(f\"ðŸ“‚ Loading {DATASET_CHOICE.upper()} dataset...\")\n",
        "\n",
        "    if CONFIG['split_type'] == \"pre_split\":\n",
        "        return load_dataset_plants()\n",
        "    elif CONFIG['split_type'] == \"70_10_20\":\n",
        "        return load_dataset_aid()\n",
        "    elif CONFIG['split_type'] == \"lc25000_split\":\n",
        "        return load_dataset_lc25000()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown split type: {CONFIG['split_type']}\")\n",
        "\n",
        "print(\"âœ… Data loading functions defined!\")\n",
        "\n",
        "# Cell 5: Feature Extraction Utilities\n",
        "# ====================================\n",
        "def preprocess_single_image_optimized(img):\n",
        "    \"\"\"Enhanced preprocessing for VGG16 with simple augmentation (horizontal flip)\"\"\"\n",
        "    resized_img = cv2.resize(img, VGG_TARGET_SIZE)\n",
        "    # Simple data augmentation for better feature generalization\n",
        "    if random.random() > 0.5:\n",
        "        resized_img = cv2.flip(resized_img, 1)\n",
        "    processed_img = preprocess_input(resized_img.astype('float32'))\n",
        "    return processed_img\n",
        "\n",
        "def extract_features_optimized(X, dataset_name, batch_size=32):\n",
        "    \"\"\"Optimized feature extraction using VGG16\"\"\"\n",
        "    feature_file = os.path.join(CONFIG['features_dir'], f\"{dataset_name}_features.pkl\")\n",
        "\n",
        "    if os.path.exists(feature_file):\n",
        "        print(f\"ðŸ“ Loading cached features for {dataset_name}...\")\n",
        "        with open(feature_file, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    print(f\"ðŸ”§ Extracting features for {dataset_name}...\")\n",
        "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "    feature_extractor = tf.keras.Sequential([\n",
        "        base_model,\n",
        "        tf.keras.layers.GlobalAveragePooling2D()\n",
        "    ])\n",
        "\n",
        "    features = []\n",
        "    total_batches = (len(X) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        batch_images = []\n",
        "        for img in X[i:i+batch_size]:\n",
        "            processed_img = preprocess_single_image_optimized(img)\n",
        "            batch_images.append(processed_img)\n",
        "\n",
        "        batch = np.array(batch_images)\n",
        "        batch_features = feature_extractor.predict(batch, verbose=0)\n",
        "        features.append(batch_features)\n",
        "\n",
        "        # Progress tracking\n",
        "        batch_num = i // batch_size + 1\n",
        "        if batch_num % 10 == 0 or batch_num == total_batches:\n",
        "            print(f\"    Processed {batch_num}/{total_batches} batches\")\n",
        "\n",
        "        del batch, batch_images\n",
        "        gc.collect()\n",
        "\n",
        "    features = np.vstack(features)\n",
        "\n",
        "    # Save features\n",
        "    with open(feature_file, 'wb') as f:\n",
        "        pickle.dump(features, f)\n",
        "\n",
        "    # Cleanup\n",
        "    del feature_extractor, base_model\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"ðŸ’¾ Saved features for {dataset_name}: {features.shape}\")\n",
        "    return features\n",
        "\n",
        "def load_and_extract_features():\n",
        "    \"\"\"Complete feature extraction pipeline\"\"\"\n",
        "    print(\"ðŸš€ Starting feature extraction pipeline...\")\n",
        "\n",
        "    # Load datasets\n",
        "    X_train_img, y_train, X_val_img, y_val, X_test_img, y_test, mapping = load_datasets()\n",
        "\n",
        "    print(f\"ðŸŽ¯ Classes found: {len(mapping)}\")\n",
        "    print(f\"ðŸ“Š Dataset sizes - Train: {X_train_img.shape[0]}, Val: {X_val_img.shape[0]}, Test: {X_test_img.shape[0]}\")\n",
        "\n",
        "    # Extract features\n",
        "    X_train_feats = extract_features_optimized(X_train_img, \"train\", batch_size=FEATURE_EXTRACTION_BATCH_SIZE)\n",
        "    X_val_feats = extract_features_optimized(X_val_img, \"val\", batch_size=FEATURE_EXTRACTION_BATCH_SIZE)\n",
        "    X_test_feats = extract_features_optimized(X_test_img, \"test\", batch_size=FEATURE_EXTRACTION_BATCH_SIZE)\n",
        "\n",
        "    # Clean up image data\n",
        "    del X_train_img, X_val_img, X_test_img\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"âœ… Feature extraction completed!\")\n",
        "    return X_train_feats, y_train, X_val_feats, y_val, X_test_feats, y_test, mapping\n",
        "\n",
        "print(\"âœ… Feature extraction utilities defined!\")\n",
        "\n",
        "# Cell 6: Structured Sparsity Norm (SSN) Implementation\n",
        "# ====================================================\n",
        "class StructuredSparsityNorm(tf.keras.regularizers.Regularizer):\n",
        "    def __init__(self, lambda1=1e-4, lambda2=1e-4):\n",
        "        self.lambda1 = lambda1\n",
        "        self.lambda2 = lambda2\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # ||W||_1,1-norm: sum of all absolute weights\n",
        "        l11_norm = self.lambda1 * tf.reduce_sum(tf.abs(x))\n",
        "\n",
        "        # ||W||_2,1-norm: sum of the Euclidean norm of each row (column group)\n",
        "        # Using axis=1 assumes features (inputs) are columns, and output units are rows.\n",
        "        # This implementation assumes weight matrix W has shape (input_dim, hidden_units)\n",
        "        l2_norm = tf.sqrt(tf.reduce_sum(tf.square(x), axis=0) + 1e-8) # L2 norm across input dimension\n",
        "        l21_norm = self.lambda2 * tf.reduce_sum(l2_norm)\n",
        "\n",
        "        return l11_norm + l21_norm\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"lambda1\": self.lambda1, \"lambda2\": self.lambda2}\n",
        "\n",
        "def build_ssn_perceptron(input_dim, n_classes, hidden_units=512, dropout_rate=0.5, lr=1e-3):\n",
        "    \"\"\"Build ANN with Structured Sparsity Norm regularization\"\"\"\n",
        "    tf.keras.backend.clear_session()\n",
        "    ssn_reg = StructuredSparsityNorm(lambda1=1e-4, lambda2=1e-4)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(input_dim,)))\n",
        "    model.add(Dense(hidden_units, activation='relu', kernel_regularizer=ssn_reg))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(hidden_units//2, activation='relu', kernel_regularizer=ssn_reg))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=lr),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def evaluate_with_ssn(model, X_val, y_val):\n",
        "    \"\"\"Evaluate feature subset using Structured Sparsity Norm concept\"\"\"\n",
        "    preds_proba = model.predict(X_val, verbose=0)\n",
        "    preds = np.argmax(preds_proba, axis=1)\n",
        "    classification_error = 1 - accuracy_score(y_val.flatten(), preds)\n",
        "\n",
        "    # Get model weights for SSN evaluation\n",
        "    weights = []\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'kernel') and layer.kernel is not None:\n",
        "            weights.append(layer.kernel.numpy())\n",
        "\n",
        "    if len(weights) > 0:\n",
        "        # We only consider the SSN on the first layer's weights for feature selection context\n",
        "        weight_matrix = weights[0]\n",
        "        l11_penalty = np.sum(np.abs(weight_matrix))\n",
        "        # Note: Axis=0 here corresponds to L2 norm of the groups (rows in the SSN table image, columns in Keras weights)\n",
        "        l21_penalty = np.sum(np.sqrt(np.sum(weight_matrix**2, axis=0)))\n",
        "\n",
        "        # SSN Evaluation = Classification Error + Regularization Penalty\n",
        "        ssn_evaluation = classification_error + 1e-4 * (l11_penalty + l21_penalty)\n",
        "    else:\n",
        "        ssn_evaluation = classification_error\n",
        "\n",
        "    return ssn_evaluation, classification_error\n",
        "\n",
        "print(\"âœ… SSN implementation complete!\")\n",
        "\n",
        "# Cell 7: GWO-ANN-SSN Feature Selector Class (Replacing GA)\n",
        "# =========================================================\n",
        "\n",
        "class GWO_ANN_SSN_FeatureSelector:\n",
        "    def __init__(self, population_size=20, max_generations=100):\n",
        "        self.population_size = population_size\n",
        "        self.max_generations = max_generations\n",
        "        self.best_alpha_pos = None\n",
        "\n",
        "    def initialize_population(self, n_features):\n",
        "        \"\"\"Initializes the population (gray wolves) randomly in continuous space [0, 1].\"\"\"\n",
        "        population = np.random.rand(self.population_size, n_features)\n",
        "        return population\n",
        "\n",
        "    def select_feature_subset(self, chromosome, X_data):\n",
        "        \"\"\"Converts the continuous wolf position to a binary feature mask using a threshold.\"\"\"\n",
        "        # Binarization using a simple threshold\n",
        "        binary_mask = (chromosome > 0.5).astype(int)\n",
        "        selected_indices = np.where(binary_mask == 1)[0]\n",
        "\n",
        "        if len(selected_indices) == 0:\n",
        "            # Fallback: select a random single feature if none are selected\n",
        "            selected_indices = [np.random.randint(0, X_data.shape[1])]\n",
        "            binary_mask = np.zeros(X_data.shape[1], dtype=int)\n",
        "            binary_mask[selected_indices[0]] = 1\n",
        "\n",
        "        return X_data[:, selected_indices], selected_indices, binary_mask\n",
        "\n",
        "    def classify_with_perceptron(self, X_train, y_train, X_val, y_val, n_classes, selected_indices):\n",
        "        \"\"\"Wrapper for training the ANN-SSN model on the feature subset.\"\"\"\n",
        "        if len(selected_indices) == 0:\n",
        "            return None, 1.0\n",
        "\n",
        "        try:\n",
        "            model = build_ssn_perceptron(len(selected_indices), n_classes, hidden_units=256)\n",
        "            early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n",
        "\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                validation_data=(X_val, y_val),\n",
        "                epochs=EVAL_EPOCHS,\n",
        "                batch_size=32,\n",
        "                verbose=0,\n",
        "                callbacks=[early_stop]\n",
        "            )\n",
        "            return model, history\n",
        "        except Exception as e:\n",
        "            # print(f\"âš ï¸ Model training failed: {e}\")\n",
        "            return None, 1.0\n",
        "\n",
        "    def evaluate_with_ssn_norm(self, model, X_val, y_val, feature_ratio):\n",
        "        \"\"\"Calculates the fitness value using classification error and feature ratio.\"\"\"\n",
        "        if model is None:\n",
        "            return 1.0 # Max fitness (worst score)\n",
        "\n",
        "        try:\n",
        "            ssn_score, classification_error = evaluate_with_ssn(model, X_val, y_val)\n",
        "\n",
        "            # Fitness = w1 * Error + w2 * Feature_Ratio\n",
        "            if DATASET_CHOICE == \"aid\":\n",
        "                 fitness = 0.9 * classification_error + 0.1 * feature_ratio\n",
        "            else:\n",
        "                 fitness = 0.7 * classification_error + 0.3 * feature_ratio\n",
        "            return fitness\n",
        "        except Exception as e:\n",
        "            # print(f\"âš ï¸ Evaluation failed: {e}\")\n",
        "            return 1.0\n",
        "\n",
        "    def update_wolf_position(self, current_pos, alpha_pos, beta_pos, delta_pos, a, n_features):\n",
        "        \"\"\"Updates the position of a wolf based on the alpha, beta, and delta positions (GWO core).\"\"\"\n",
        "\n",
        "        def calculate_new_position_component(leader_pos, current_pos, a, n_features):\n",
        "            C = 2 * np.random.rand(n_features)\n",
        "            A = 2 * a * np.random.rand(n_features) - a\n",
        "\n",
        "            D = np.abs(C * leader_pos - current_pos)\n",
        "            X1 = leader_pos - A * D\n",
        "            return X1\n",
        "\n",
        "        # Calculate steps towards Alpha, Beta, and Delta wolves\n",
        "        X_alpha = calculate_new_position_component(alpha_pos, current_pos, a, n_features)\n",
        "        X_beta = calculate_new_position_component(beta_pos, current_pos, a, n_features)\n",
        "        X_delta = calculate_new_position_component(delta_pos, current_pos, a, n_features)\n",
        "\n",
        "        # New position is the average of the steps\n",
        "        new_pos = (X_alpha + X_beta + X_delta) / 3\n",
        "\n",
        "        # Clamp positions to the search space [0, 1]\n",
        "        new_pos = np.clip(new_pos, 0, 1)\n",
        "        return new_pos\n",
        "\n",
        "    def optimize(self, X_train, y_train, X_val, y_val, n_classes, n_features):\n",
        "        print(\"ðŸš€ Starting GWO-ANN-SSN Feature Selection\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        population = self.initialize_population(n_features)\n",
        "\n",
        "        alpha_pos, beta_pos, delta_pos = None, None, None\n",
        "        alpha_fitness, beta_fitness, delta_fitness = float('inf'), float('inf'), float('inf')\n",
        "        best_accuracy = 0.0\n",
        "        no_improvement_count = 0\n",
        "\n",
        "        for generation in range(self.max_generations):\n",
        "            print(f\"\\nðŸ”„ Generation {generation + 1}/{self.max_generations}\")\n",
        "            fitness_values = []\n",
        "            current_population_masks = []\n",
        "\n",
        "            # 'a' decreases linearly from 2 to 0\n",
        "            a = 2 - generation * (2 / self.max_generations)\n",
        "\n",
        "            # Step 1: Evaluate current population\n",
        "            for i, chromosome in enumerate(population):\n",
        "                X_train_subset, selected_indices, binary_mask = self.select_feature_subset(chromosome, X_train)\n",
        "                X_val_subset, _, _ = self.select_feature_subset(chromosome, X_val)\n",
        "                feature_ratio = len(selected_indices) / n_features\n",
        "\n",
        "                model, _ = self.classify_with_perceptron(X_train_subset, y_train, X_val_subset, y_val, n_classes, selected_indices)\n",
        "                fitness = self.evaluate_with_ssn_norm(model, X_val_subset, y_val, feature_ratio)\n",
        "\n",
        "                fitness_values.append(fitness)\n",
        "                current_population_masks.append(binary_mask)\n",
        "\n",
        "                if model is not None:\n",
        "                    del model\n",
        "                gc.collect()\n",
        "\n",
        "            fitness_array = np.array(fitness_values)\n",
        "\n",
        "            # Step 2: Identify Alpha, Beta, Delta (the top 3 best solutions)\n",
        "            sorted_indices = np.argsort(fitness_array)\n",
        "\n",
        "            # Update leaders and check for global improvement\n",
        "            new_alpha_fitness = fitness_array[sorted_indices[0]]\n",
        "            if new_alpha_fitness < alpha_fitness:\n",
        "                alpha_fitness = new_alpha_fitness\n",
        "                alpha_pos = population[sorted_indices[0]].copy()\n",
        "                alpha_mask = current_population_masks[sorted_indices[0]]\n",
        "                no_improvement_count = 0\n",
        "\n",
        "                # Recalculate and print best accuracy\n",
        "                try:\n",
        "                    X_val_subset, selected_indices, _ = self.select_feature_subset(alpha_pos, X_val)\n",
        "                    current_model, _ = self.classify_with_perceptron(X_train[:, alpha_mask==1], y_train, X_val_subset, y_val, n_classes, selected_indices)\n",
        "                    if current_model:\n",
        "                        preds = np.argmax(current_model.predict(X_val_subset, verbose=0), axis=1)\n",
        "                        best_accuracy = accuracy_score(y_val.flatten(), preds)\n",
        "                        del current_model\n",
        "                        print(f\" Â  âœ… New alpha! Fitness: {alpha_fitness:.4f}, Accuracy: {best_accuracy:.4f}, Features: {len(selected_indices)}\")\n",
        "                except:\n",
        "                    print(f\" Â  âœ… New alpha! Fitness: {alpha_fitness:.4f}, Features: {len(selected_indices)} (accuracy calc failed)\")\n",
        "\n",
        "            else:\n",
        "                no_improvement_count += 1\n",
        "\n",
        "            # Ensure at least three leaders exist for position update\n",
        "            if len(sorted_indices) >= 3:\n",
        "                beta_fitness = fitness_array[sorted_indices[1]]\n",
        "                beta_pos = population[sorted_indices[1]].copy()\n",
        "                delta_fitness = fitness_array[sorted_indices[2]]\n",
        "                delta_pos = population[sorted_indices[2]].copy()\n",
        "            else:\n",
        "                # Fallback if population size is too small (should not happen with default parameters)\n",
        "                beta_pos = alpha_pos.copy() if alpha_pos is not None else np.random.rand(n_features)\n",
        "                delta_pos = alpha_pos.copy() if alpha_pos is not None else np.random.rand(n_features)\n",
        "\n",
        "            # Check termination\n",
        "            if (generation >= self.max_generations or no_improvement_count >= 30):\n",
        "                break\n",
        "\n",
        "            # Ensure leaders are not None before update\n",
        "            if alpha_pos is None or beta_pos is None or delta_pos is None:\n",
        "                # If leaders are not initialized (e.g., all models failed in Gen 1), re-initialize population\n",
        "                population = self.initialize_population(n_features)\n",
        "                continue\n",
        "\n",
        "            # Step 3: Update positions of all other wolves\n",
        "            new_population = []\n",
        "            for i, current_pos in enumerate(population):\n",
        "                # Elitism: keep the best three wolves\n",
        "                if i in sorted_indices[:3]:\n",
        "                    new_population.append(current_pos.copy())\n",
        "                else:\n",
        "                    new_pos = self.update_wolf_position(current_pos, alpha_pos, beta_pos, delta_pos, a, n_features)\n",
        "                    new_population.append(new_pos)\n",
        "\n",
        "            population = np.array(new_population)\n",
        "\n",
        "            avg_fitness, current_best = np.mean(fitness_array), np.min(fitness_array)\n",
        "            population_diversity = np.mean(np.std(population > 0.5, axis=0))\n",
        "            print(f\" Â  ðŸ“Š Stats - Best: {current_best:.4f}, Avg: {avg_fitness:.4f}, Diversity: {population_diversity:.4f}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"ðŸŽ¯ GWO-ANN-SSN COMPLETED\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        if alpha_pos is None:\n",
        "             # Fallback: select all features if GWO failed\n",
        "             alpha_pos = np.ones(n_features)\n",
        "             alpha_fitness = 1.0\n",
        "\n",
        "        # Final best features are the binarized alpha position\n",
        "        X_final_subset, final_selected_indices, final_best_mask = self.select_feature_subset(alpha_pos, X_train)\n",
        "\n",
        "        print(f\"Best Fitness: {alpha_fitness:.4f}\")\n",
        "        print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
        "        print(f\"Selected Features: {len(final_selected_indices)}/{n_features}\")\n",
        "        print(f\"Final Generation: {generation + 1}\")\n",
        "\n",
        "        return final_best_mask, alpha_fitness, best_accuracy, final_selected_indices\n",
        "\n",
        "print(\"âœ… GWO-ANN-SSN Feature Selector class defined!\")\n",
        "\n",
        "\n",
        "# Cell 8: Main Pipeline Execution\n",
        "# ===============================\n",
        "print(\"ðŸš€ Starting Unified GWO-ANN-SSN Pipeline\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Step 1: Load and extract features\n",
        "X_train_feats, y_train, X_val_feats, y_val, X_test_feats, y_test, mapping = load_and_extract_features()\n",
        "n_classes = len(mapping)\n",
        "\n",
        "# Step 2: Feature standardization\n",
        "print(\"ðŸ”§ Standardizing features...\")\n",
        "scaler = StandardScaler()\n",
        "X_train_feats = scaler.fit_transform(X_train_feats)\n",
        "X_val_feats = scaler.transform(X_val_feats)\n",
        "X_test_feats = scaler.transform(X_test_feats)\n",
        "\n",
        "# Step 3: Apply PCA\n",
        "print(\"ðŸ“Š Applying PCA...\")\n",
        "pca_subset_size = min(2000, len(X_train_feats))\n",
        "pca_indices = np.random.choice(len(X_train_feats), pca_subset_size, replace=False)\n",
        "\n",
        "pca_temp = PCA(n_components=PCA_VARIANCE, random_state=RANDOM_SEED)\n",
        "pca_temp.fit(X_train_feats[pca_indices])\n",
        "n_components_95 = pca_temp.n_components_\n",
        "\n",
        "pca = PCA(n_components=n_components_95, random_state=RANDOM_SEED)\n",
        "pca.fit(X_train_feats[pca_indices])\n",
        "\n",
        "X_train_pca = pca.transform(X_train_feats)\n",
        "X_val_pca = pca.transform(X_val_feats)\n",
        "X_test_pca = pca.transform(X_test_feats)\n",
        "\n",
        "n_features = X_train_pca.shape[1]\n",
        "print(f\"âœ… PCA completed: {n_features} features (from {X_train_feats.shape[1]})\")\n",
        "\n",
        "# Save PCA and scaler\n",
        "pca_file = os.path.join(CONFIG['features_dir'], \"pca_scaler.pkl\")\n",
        "with open(pca_file, 'wb') as f:\n",
        "    pickle.dump({'pca': pca, 'scaler': scaler}, f)\n",
        "print(\"ðŸ’¾ Saved PCA and Scaler\")\n",
        "\n",
        "print(\"âœ… Data preprocessing completed!\")\n",
        "\n",
        "# Cell 9: EFFICIENT GWO-ANN-SSN Feature Selection\n",
        "# ==============================================\n",
        "print(\"ðŸš€ Starting EFFICIENT GWO-ANN-SSN Feature Selection...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Adjusted parameters for a better trade-off between efficiency and performance\n",
        "EFFICIENT_POPULATION_SIZE = 8\n",
        "EFFICIENT_MAX_GENERATION = 10\n",
        "\n",
        "EFFICIENT_EVAL_EPOCHS = 5 # Increased from 3 to 5 for better evaluation\n",
        "EVAL_EPOCHS = EFFICIENT_EVAL_EPOCHS  # Override for efficiency\n",
        "\n",
        "print(f\"âš¡ RUNNING IN EFFICIENT MODE âš¡\")\n",
        "print(f\" Â  â€¢ Population: {EFFICIENT_POPULATION_SIZE}\")\n",
        "print(f\" Â  â€¢ Generations: {EFFICIENT_MAX_GENERATION}\")\n",
        "print(f\" Â  â€¢ Evaluation Epochs: {EVAL_EPOCHS}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# --- Instantiation changed to GWO selector ---\n",
        "gwo_selector = GWO_ANN_SSN_FeatureSelector(\n",
        "    population_size=EFFICIENT_POPULATION_SIZE,\n",
        "    max_generations=EFFICIENT_MAX_GENERATION\n",
        ")\n",
        "\n",
        "try:\n",
        "    best_mask, best_fitness, best_accuracy, selected_indices = gwo_selector.optimize(\n",
        "        X_train_pca, y_train, X_val_pca, y_val, n_classes, n_features\n",
        "    )\n",
        "    print(\"âœ… GWO-ANN-SSN feature selection completed!\")\n",
        "\n",
        "except UnboundLocalError as e:\n",
        "    print(f\"âŒ Error in GWO optimization: {e}\")\n",
        "    print(\"ðŸ”„ Creating fallback solution (Variance Threshold)...\")\n",
        "    from sklearn.feature_selection import VarianceThreshold\n",
        "    selector = VarianceThreshold()\n",
        "    selector.fit(X_train_pca)\n",
        "    variances = selector.variances_\n",
        "    n_select = max(1, n_features // 2)\n",
        "    selected_indices = np.argsort(variances)[-n_select:]\n",
        "    best_mask = np.zeros(n_features)\n",
        "    best_mask[selected_indices] = 1\n",
        "    best_fitness, best_accuracy = 0.5, 0.5\n",
        "    print(f\"ðŸ”„ Using fallback: {len(selected_indices)} features selected\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Unexpected error in GWO: {e}\")\n",
        "    print(\"ðŸ”„ Creating random feature selection as fallback...\")\n",
        "    n_select = max(1, n_features // 2)\n",
        "    selected_indices = np.random.choice(n_features, n_select, replace=False)\n",
        "    best_mask = np.zeros(n_features)\n",
        "    best_mask[selected_indices] = 1\n",
        "    best_fitness, best_accuracy = 0.5, 0.5\n",
        "    print(f\"ðŸ”„ Using random selection: {len(selected_indices)} features selected\")\n",
        "\n",
        "print(f\"ðŸŽ¯ Final selection: {len(selected_indices)} features out of {n_features}\")\n",
        "\n",
        "# Restore original value\n",
        "EVAL_EPOCHS = 10\n",
        "print(f\"\\nâ„¹ï¸ Restored EVAL_EPOCHS to original value ({EVAL_EPOCHS})\")\n",
        "\n",
        "# Cell 10: Final Model Training\n",
        "# =============================\n",
        "print(\"\\nðŸŽ¯ Training Final Model with Selected Features...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Prepare data with selected features\n",
        "X_train_final = np.vstack([X_train_pca, X_val_pca])[:, selected_indices]\n",
        "y_train_final = np.vstack([y_train, y_val])\n",
        "X_test_final = X_test_pca[:, selected_indices]\n",
        "\n",
        "print(f\"ðŸ“Š Final training set: {X_train_final.shape}\")\n",
        "print(f\"ðŸ“Š Test set: {X_test_final.shape}\")\n",
        "print(f\"ðŸ”§ Selected features: {X_train_final.shape[1]}\")\n",
        "\n",
        "# Build and train final model\n",
        "final_model = build_ssn_perceptron(X_train_final.shape[1], n_classes, hidden_units=512, dropout_rate=0.5)\n",
        "\n",
        "print(f\"ðŸ”§ Training final model on combined train+val data for {FINAL_EPOCHS} epochs...\")\n",
        "history = final_model.fit(\n",
        "    X_train_final, y_train_final,\n",
        "    epochs=FINAL_EPOCHS, # FINAL_EPOCHS is now 100\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1,\n",
        "    validation_split=0.0  # No validation during final training\n",
        ")\n",
        "\n",
        "print(\"âœ… Final model training completed!\")\n",
        "\n",
        "# Cell 11: Evaluation and Results Summary\n",
        "# =======================================\n",
        "print(\"\\nðŸ“ˆ Evaluating on completely held-out TEST set...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Validation performance for reference\n",
        "X_val_selected = X_val_pca[:, selected_indices]\n",
        "val_preds = np.argmax(final_model.predict(X_val_selected, verbose=0), axis=1)\n",
        "val_accuracy = accuracy_score(y_val.flatten(), val_preds)\n",
        "\n",
        "# Test performance (true performance)\n",
        "test_preds = np.argmax(final_model.predict(X_test_final, verbose=0), axis=1)\n",
        "test_accuracy = accuracy_score(y_test.flatten(), test_preds)\n",
        "\n",
        "print(f\"ðŸ† VALIDATION Accuracy (for reference): {val_accuracy:.4f}\")\n",
        "print(f\"ðŸŽ¯ TEST Accuracy (true performance): {test_accuracy:.4f}\")\n",
        "\n",
        "# Final results summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"ðŸŽ¯ FINAL GWO-ANN-SSN RESULTS - {DATASET_CHOICE.upper()}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"ðŸ“Š Dataset Usage:\")\n",
        "print(f\" Â  â€¢ Train: {X_train_pca.shape[0]} samples (feature selection)\")\n",
        "print(f\" Â  â€¢ Validation: {X_val_pca.shape[0]} samples (fitness evaluation)\")\n",
        "print(f\" Â  â€¢ Test: {X_test_pca.shape[0]} samples (FINAL evaluation)\")\n",
        "print(f\" Â  â€¢ Final Training: {X_train_final.shape[0]} samples (train+val)\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Performance Metrics:\")\n",
        "print(f\" Â  â€¢ Best Fitness: {best_fitness:.4f}\")\n",
        "print(f\" Â  â€¢ Validation Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\" Â  â€¢ ðŸ† TEST Accuracy: {test_accuracy:.4f} â† TRUE PERFORMANCE\")\n",
        "\n",
        "print(f\"\\nðŸ”§ Feature Selection:\")\n",
        "print(f\" Â  â€¢ Selected Features: {len(selected_indices)}/{n_features}\")\n",
        "print(f\" Â  â€¢ Feature Reduction: {((n_features - len(selected_indices)) / n_features * 100):.1f}%\")\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'dataset': DATASET_CHOICE,\n",
        "    'test_accuracy': test_accuracy,\n",
        "    'validation_accuracy': val_accuracy,\n",
        "    'gwo_best_fitness': best_fitness,\n",
        "    'selected_features_count': len(selected_indices),\n",
        "    'total_features': n_features,\n",
        "    'selected_indices': selected_indices,\n",
        "    'dataset_sizes': {\n",
        "        'train': X_train_pca.shape[0],\n",
        "        'validation': X_val_pca.shape[0],\n",
        "        'test': X_test_pca.shape[0],\n",
        "        'final_training': X_train_final.shape[0]\n",
        "    },\n",
        "    'classes': mapping\n",
        "}\n",
        "\n",
        "results_file = os.path.join(CONFIG['features_dir'], \"gwo_ann_ssn_results.pkl\")\n",
        "with open(results_file, 'wb') as f:\n",
        "    pickle.dump(results, f)\n",
        "\n",
        "print(f\"\\nðŸ’¾ Results saved to: {results_file}\")\n",
        "print(\"âœ… Unified pipeline completed successfully!\")\n",
        "\n",
        "# Cell 12: Results Consolidation\n",
        "# ==============================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ðŸ“Š RESULTS CONSOLIDATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def save_consolidated_results(test_accuracy, val_accuracy, best_fitness, selected_indices,\n",
        "                              n_features, n_classes, mapping, X_train_pca, X_test_pca):\n",
        "    \"\"\"Save all results to consolidated files\"\"\"\n",
        "\n",
        "    # Create results entry for current dataset\n",
        "    results_entry = {\n",
        "        'dataset': DATASET_CHOICE,\n",
        "        'performance_metrics': {\n",
        "            'test_accuracy': float(test_accuracy),\n",
        "            'validation_accuracy': float(val_accuracy),\n",
        "            'best_fitness': float(best_fitness),\n",
        "            'performance_gap': float(test_accuracy - val_accuracy)\n",
        "        },\n",
        "        'feature_selection': {\n",
        "            'selected_features_count': len(selected_indices),\n",
        "            'total_features': n_features,\n",
        "            'feature_reduction_percentage': float((n_features - len(selected_indices)) / n_features * 100),\n",
        "            'selected_features_ratio': float(len(selected_indices) / n_features)\n",
        "        },\n",
        "        'dataset_info': {\n",
        "            'num_classes': n_classes,\n",
        "            'class_names': list(mapping.values()),\n",
        "            'training_samples': int(X_train_pca.shape[0]),\n",
        "            'test_samples': int(X_test_pca.shape[0])\n",
        "        },\n",
        "        'timestamp': str(pd.Timestamp.now())\n",
        "    }\n",
        "\n",
        "    # 1. Append to main JSON results file\n",
        "    main_json_file = \"gwo_ann_ssn_datasets_results.json\"\n",
        "\n",
        "    # Load existing results or create new\n",
        "    if os.path.exists(main_json_file):\n",
        "        with open(main_json_file, 'r') as f:\n",
        "            all_results = json.load(f)\n",
        "    else:\n",
        "        all_results = {}\n",
        "\n",
        "    # Add/update current dataset results\n",
        "    all_results[DATASET_CHOICE] = results_entry\n",
        "\n",
        "    # Save updated results\n",
        "    with open(main_json_file, 'w') as f:\n",
        "        json.dump(all_results, f, indent=4)\n",
        "\n",
        "    print(f\"ðŸ’¾ Results appended to: {main_json_file}\")\n",
        "\n",
        "    # 2. Update summary text file\n",
        "    summary_file = \"gwo_ann_ssn_datasets_summary.txt\"\n",
        "\n",
        "    with open(summary_file, 'w') as f:\n",
        "        f.write(\"=\" * 70 + \"\\n\")\n",
        "        f.write(\"GWO-ANN-SSN ALL DATASETS SUMMARY\\n\")\n",
        "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
        "\n",
        "        # Write header\n",
        "        f.write(f\"{'Dataset':<12} {'Test Acc':<10} {'Val Acc':<10} {'Features':<12} {'Reduction':<12} {'Classes':<8}\\n\")\n",
        "        f.write(\"-\" * 70 + \"\\n\")\n",
        "\n",
        "        # Write all dataset entries\n",
        "        for dataset_name, dataset_results in all_results.items():\n",
        "            perf = dataset_results['performance_metrics']\n",
        "            feat = dataset_results['feature_selection']\n",
        "            info = dataset_results['dataset_info']\n",
        "\n",
        "            f.write(f\"{dataset_name:<12} {perf['test_accuracy']:<10.4f} {perf['validation_accuracy']:<10.4f} \"\n",
        "                    f\"{feat['selected_features_count']}/{feat['total_features']:<12} \"\n",
        "                    f\"{feat['feature_reduction_percentage']:<12.1f}% \"\n",
        "                    f\"{info['num_classes']:<8}\\n\")\n",
        "\n",
        "        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
        "        f.write(\"LATEST RUN DETAILS:\\n\")\n",
        "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
        "\n",
        "        # Add details for current run\n",
        "        f.write(f\"Dataset: {DATASET_CHOICE}\\n\")\n",
        "        f.write(f\"Test Accuracy: {test_accuracy:.4f}\\n\")\n",
        "        f.write(f\"Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
        "        f.write(f\"Selected Features: {len(selected_indices)}/{n_features}\\n\")\n",
        "        f.write(f\"Feature Reduction: {((n_features - len(selected_indices)) / n_features * 100):.1f}%\\n\")\n",
        "        f.write(f\"Number of Classes: {n_classes}\\n\")\n",
        "        f.write(f\"Best Fitness: {best_fitness:.4f}\\n\")\n",
        "        f.write(f\"Classes: {', '.join(list(mapping.values()))}\\n\")\n",
        "        f.write(f\"Timestamp: {results_entry['timestamp']}\\n\")\n",
        "\n",
        "    print(f\"ðŸ“‹ Summary updated in: {summary_file}\")\n",
        "\n",
        "    # Print current results to console\n",
        "    print(f\"\\nðŸ“Š {DATASET_CHOICE.upper()} RESULTS:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Test Accuracy:       {test_accuracy:.4f}\")\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    print(f\"Selected Features:   {len(selected_indices)}/{n_features}\")\n",
        "    print(f\"Feature Reduction:   {((n_features - len(selected_indices)) / n_features * 100):.1f}%\")\n",
        "    print(f\"Best Fitness:        {best_fitness:.4f}\")\n",
        "\n",
        "    return results_entry\n",
        "\n",
        "# Execute results consolidation\n",
        "try:\n",
        "    consolidated_results = save_consolidated_results(\n",
        "        test_accuracy, val_accuracy, best_fitness, selected_indices,\n",
        "        n_features, n_classes, mapping, X_train_pca, X_test_pca\n",
        "    )\n",
        "    print(\"\\nâœ… All results consolidated successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Error in consolidating results: {e}\")\n",
        "    print(\"Continuing with existing results...\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hqgeV8cDSb38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bchQV4rJHsBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78538535-7818-4edb-c66a-cb811a1de72a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ðŸ“„ CONSOLIDATED RESULTS SUMMARY (TXT)\n",
            "============================================================\n",
            "======================================================================\n",
            "GWO-ANN-SSN ALL DATASETS SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Dataset      Test Acc   Val Acc    Features     Reduction    Classes \n",
            "----------------------------------------------------------------------\n",
            "plants       0.9260     0.9809     142/331          57.1        % 30      \n",
            "aid          0.7300     0.9980     132/327          59.6        % 30      \n",
            "lc25000      0.9768     0.9968     124/325          61.8        % 5       \n",
            "\n",
            "======================================================================\n",
            "LATEST RUN DETAILS:\n",
            "======================================================================\n",
            "\n",
            "Dataset: lc25000\n",
            "Test Accuracy: 0.9768\n",
            "Validation Accuracy: 0.9968\n",
            "Selected Features: 124/325\n",
            "Feature Reduction: 61.8%\n",
            "Number of Classes: 5\n",
            "Best Fitness: 0.1363\n",
            "Classes: colon_aca, colon_n, lung_aca, lung_n, lung_scc\n",
            "Timestamp: 2025-10-31 12:27:25.334730\n",
            "\n",
            "\n",
            "============================================================\n",
            "ðŸ’¡ FULL JSON RESULTS (DETAIL)\n",
            "============================================================\n",
            "{\n",
            "    \"plants\": {\n",
            "        \"dataset\": \"plants\",\n",
            "        \"performance_metrics\": {\n",
            "            \"test_accuracy\": 0.9259506337558372,\n",
            "            \"validation_accuracy\": 0.9808580858085808,\n",
            "            \"best_fitness\": 0.24883291954573095,\n",
            "            \"performance_gap\": -0.05490745205274361\n",
            "        },\n",
            "        \"feature_selection\": {\n",
            "            \"selected_features_count\": 142,\n",
            "            \"total_features\": 331,\n",
            "            \"feature_reduction_percentage\": 57.09969788519638,\n",
            "            \"selected_features_ratio\": 0.42900302114803623\n",
            "        },\n",
            "        \"dataset_info\": {\n",
            "            \"num_classes\": 30,\n",
            "            \"class_names\": [\n",
            "                \"aloevera\",\n",
            "                \"banana\",\n",
            "                \"bilimbi\",\n",
            "                \"cantaloupe\",\n",
            "                \"cassava\",\n",
            "                \"coconut\",\n",
            "                \"corn\",\n",
            "                \"cucumber\",\n",
            "                \"curcuma\",\n",
            "                \"eggplant\",\n",
            "                \"galangal\",\n",
            "                \"ginger\",\n",
            "                \"guava\",\n",
            "                \"kale\",\n",
            "                \"longbeans\",\n",
            "                \"mango\",\n",
            "                \"melon\",\n",
            "                \"orange\",\n",
            "                \"paddy\",\n",
            "                \"papaya\",\n",
            "                \"peper chili\",\n",
            "                \"pineapple\",\n",
            "                \"pomelo\",\n",
            "                \"shallot\",\n",
            "                \"soybeans\",\n",
            "                \"spinach\",\n",
            "                \"sweet potatoes\",\n",
            "                \"tobacco\",\n",
            "                \"waterapple\",\n",
            "                \"watermelon\"\n",
            "            ],\n",
            "            \"training_samples\": 23972,\n",
            "            \"test_samples\": 2998\n",
            "        },\n",
            "        \"timestamp\": \"2025-10-31 11:24:53.709980\"\n",
            "    },\n",
            "    \"aid\": {\n",
            "        \"dataset\": \"aid\",\n",
            "        \"performance_metrics\": {\n",
            "            \"test_accuracy\": 0.73,\n",
            "            \"validation_accuracy\": 0.998,\n",
            "            \"best_fitness\": 0.3337669724770642,\n",
            "            \"performance_gap\": -0.268\n",
            "        },\n",
            "        \"feature_selection\": {\n",
            "            \"selected_features_count\": 132,\n",
            "            \"total_features\": 327,\n",
            "            \"feature_reduction_percentage\": 59.63302752293578,\n",
            "            \"selected_features_ratio\": 0.4036697247706422\n",
            "        },\n",
            "        \"dataset_info\": {\n",
            "            \"num_classes\": 30,\n",
            "            \"class_names\": [\n",
            "                \"Airport\",\n",
            "                \"BareLand\",\n",
            "                \"BaseballField\",\n",
            "                \"Beach\",\n",
            "                \"Bridge\",\n",
            "                \"Center\",\n",
            "                \"Church\",\n",
            "                \"Commercial\",\n",
            "                \"DenseResidential\",\n",
            "                \"Desert\",\n",
            "                \"Farmland\",\n",
            "                \"Forest\",\n",
            "                \"Industrial\",\n",
            "                \"Meadow\",\n",
            "                \"MediumResidential\",\n",
            "                \"Mountain\",\n",
            "                \"Park\",\n",
            "                \"Parking\",\n",
            "                \"Playground\",\n",
            "                \"Pond\",\n",
            "                \"Port\",\n",
            "                \"RailwayStation\",\n",
            "                \"Resort\",\n",
            "                \"River\",\n",
            "                \"School\",\n",
            "                \"SparseResidential\",\n",
            "                \"Square\",\n",
            "                \"Stadium\",\n",
            "                \"StorageTanks\",\n",
            "                \"Viaduct\"\n",
            "            ],\n",
            "            \"training_samples\": 2000,\n",
            "            \"test_samples\": 500\n",
            "        },\n",
            "        \"timestamp\": \"2025-10-31 11:50:34.452252\"\n",
            "    },\n",
            "    \"lc25000\": {\n",
            "        \"dataset\": \"lc25000\",\n",
            "        \"performance_metrics\": {\n",
            "            \"test_accuracy\": 0.9767907162865146,\n",
            "            \"validation_accuracy\": 0.9968,\n",
            "            \"best_fitness\": 0.13630153846153847,\n",
            "            \"performance_gap\": -0.020009283713485426\n",
            "        },\n",
            "        \"feature_selection\": {\n",
            "            \"selected_features_count\": 124,\n",
            "            \"total_features\": 325,\n",
            "            \"feature_reduction_percentage\": 61.846153846153854,\n",
            "            \"selected_features_ratio\": 0.38153846153846155\n",
            "        },\n",
            "        \"dataset_info\": {\n",
            "            \"num_classes\": 5,\n",
            "            \"class_names\": [\n",
            "                \"colon_aca\",\n",
            "                \"colon_n\",\n",
            "                \"lung_aca\",\n",
            "                \"lung_n\",\n",
            "                \"lung_scc\"\n",
            "            ],\n",
            "            \"training_samples\": 10000,\n",
            "            \"test_samples\": 2499\n",
            "        },\n",
            "        \"timestamp\": \"2025-10-31 12:27:25.334730\"\n",
            "    }\n",
            "}\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# --- Configuration (Ensure these match your saved filenames) ---\n",
        "SUMMARY_FILE = \"gwo_ann_ssn_datasets_summary.txt\"\n",
        "JSON_FILE = \"gwo_ann_ssn_datasets_results.json\"\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ðŸ“„ CONSOLIDATED RESULTS SUMMARY (TXT)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if os.path.exists(SUMMARY_FILE):\n",
        "    with open(SUMMARY_FILE, 'r') as f:\n",
        "        summary_content = f.read()\n",
        "    print(summary_content)\n",
        "else:\n",
        "    print(f\"âŒ Error: Summary file '{SUMMARY_FILE}' not found.\")\n",
        "    print(\"Please ensure the file was created in the correct directory.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ’¡ FULL JSON RESULTS (DETAIL)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if os.path.exists(JSON_FILE):\n",
        "    with open(JSON_FILE, 'r') as f:\n",
        "        json_results = json.load(f)\n",
        "\n",
        "    # Use json.dumps for neat, formatted printing\n",
        "    print(json.dumps(json_results, indent=4))\n",
        "else:\n",
        "    print(f\"âŒ Error: JSON file '{JSON_FILE}' not found.\")\n",
        "    print(\"Please ensure the file was created in the correct directory.\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    }
  ]
}